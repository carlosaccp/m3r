{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math as m\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we implement a class to represent a normal random variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalRV:\n",
    "    \"\"\"\n",
    "    Class for a multivariate normal random variable.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, mu, Sigma):\n",
    "        \"\"\"\n",
    "        Constructor. Sets:\n",
    "        - mu (np.array): mean\n",
    "        - Sigma (np.matrix): covariance matrix\n",
    "        - S (np.matrix): inverse of Sigma\n",
    "        - m (np.array): S@mu\n",
    "        \"\"\"\n",
    "        self.mu = mu\n",
    "        self.Sigma = Sigma\n",
    "        self.S = np.linalg.inv(Sigma)\n",
    "        self.m = self.S@self.mu\n",
    "    \n",
    "    def pdf(self, x):\n",
    "        \"\"\"\n",
    "        Evaluates the pdf at x. x can be a vector or a matrix; in the latter case, \n",
    "        the pdf is evaluated column-wise.\n",
    "\n",
    "        Args:\n",
    "        - x (np.array): point(s) at which to evaluate the pdf\n",
    "\n",
    "        Returns:\n",
    "        - pdf (np.array): pdf evaluated at x\n",
    "        \"\"\"\n",
    "\n",
    "        mu = self.mu\n",
    "        Sigma = self.Sigma\n",
    "        d = len(mu)\n",
    "        x = np.asarray(x)\n",
    "        if x.ndim == 1:\n",
    "            x = x[:, np.newaxis]\n",
    "        n = x.shape[1]\n",
    "        pdf = np.zeros(n)\n",
    "        for i in range(n):\n",
    "            pdf[i] = 1/np.sqrt((2*np.pi)**d * np.linalg.det(Sigma)) * \\\n",
    "                np.exp(-1/2 * (x[:,i] - mu).T @ np.linalg.inv(Sigma) @ (x[:,i] - mu))\n",
    "        return pdf\n",
    "\n",
    "    def sample(self, n):\n",
    "        \"\"\"\n",
    "        Draws n samples from the distribution.\n",
    "\n",
    "        Args:\n",
    "        - n (int): number of samples to draw\n",
    "\n",
    "        Returns:\n",
    "        - samples (np.array): samples drawn from the distribution\n",
    "        \"\"\"\n",
    "\n",
    "        mu = self.mu\n",
    "        Sigma = self.Sigma\n",
    "        return np.random.multivariate_normal(mu, Sigma, n).T\n",
    "    \n",
    "    def update_vals(self, m, S):\n",
    "        \"\"\"\n",
    "        Updates the mean and covariance matrix of the distribution.\n",
    "\n",
    "        Args:\n",
    "        - m (np.array): new mean\n",
    "        - S (np.matrix): new covariance matrix\n",
    "\n",
    "        Sets:\n",
    "        - mu (np.array): mean\n",
    "        - Sigma (np.matrix): covariance matrix\n",
    "        - S (np.matrix): inverse of Sigma\n",
    "        - m (np.array): S@mu\n",
    "        \"\"\"\n",
    "\n",
    "        self.m = m\n",
    "        self.S = S\n",
    "        self.Sigma = np.linalg.inv(S)\n",
    "        self.mu = self.Sigma@m\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that our update step is:\n",
    "\n",
    "$$\n",
    "\\theta_{n+1}\\leftarrow \\theta_n - \\alpha \\nabla \\rho(\\theta)\n",
    "$$\n",
    "\n",
    "In our case, as both distributions are normal, we can use the fundamental representation of the normal distribution so that its pdf is in the exponential family. This means that:\n",
    "\n",
    "$$\n",
    "\\nabla \\rho(\\theta) = \\int_{\\mathbb{R}^n}-\\frac{\\pi^2(x)}{q^2_\\theta(x)}\\nabla\\log q_\\theta(x) dx\n",
    "$$\n",
    "\n",
    "Then, as $\\theta = (\\mu, \\Sigma)$ and using the fundamental representation of the normal distribution we arrive to:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial \\log}{\\partial m}(x) &= x-S^{-1}m = x-\\mu\\\\\n",
    "\\frac{\\partial \\log}{\\partial S}(x) &= -\\frac{1}{2}(xx^\\top - S^{-1}mm^\\top S^{-1} - S^{-1}) = -\\frac{1}{2}(xx^\\top - \\mu\\mu^\\top - \\Sigma)\n",
    "\\end{align*}\n",
    "$$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now implement functions to compute the gradient descent update, using the equations above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partial_lq_m(x, p):\n",
    "    \"\"\"\n",
    "    Computes the partial derivative of the log-likelihood function with respect to the mean.\n",
    "\n",
    "    Args:\n",
    "    - x (np.array): point at which to evaluate the partial derivative\n",
    "    - p (NormalRV): distribution\n",
    "\n",
    "    Returns:\n",
    "    - partial (np.array): partial derivative of the log-likelihood function with respect to the mean\n",
    "    \"\"\"\n",
    "\n",
    "    partial = x - np.reshape(p.mu, (len(p.mu), 1))\n",
    "    return partial\n",
    "\n",
    "def partial_lq_S(x, p):\n",
    "    \"\"\"\n",
    "    Computes the partial derivative of the log-likelihood function with respect to the covariance matrix.\n",
    "\n",
    "    Args:\n",
    "    - x (np.array): point at which to evaluate the partial derivative\n",
    "    - p (NormalRV): distribution\n",
    "\n",
    "    Returns:\n",
    "    - res (np.array): partial derivative of the log-likelihood function with respect to the covariance matrix\n",
    "    \"\"\"\n",
    "\n",
    "    x = np.asarray(x)\n",
    "    if x.ndim == 1:\n",
    "        x = x[:, np.newaxis]\n",
    "    n = x.shape[1]\n",
    "    d = len(p.mu)\n",
    "    res = np.zeros((d,d,n))\n",
    "    for i in range(n):\n",
    "        xi = x[:,i]\n",
    "        res[:,:,i] = -0.5*(np.outer(xi, xi) - np.outer(p.mu, p.mu)- p.Sigma)\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_pd(M):\n",
    "    \"\"\"\n",
    "    Checks if matrix is in the positive definite cone using the Cholesky decomposition.\n",
    "\n",
    "    Args:\n",
    "    - M (np.matrix): matrix to check\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        _ = np.linalg.cholesky(M)\n",
    "        return True\n",
    "    except np.linalg.LinAlgError:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def project_pd(M, eps=1e-6):\n",
    "    \"\"\"\n",
    "    Projects a matrix to the positive definite cone. If the matrix is already in the positive definite cone, \n",
    "    it is returned unchanged.\n",
    "\n",
    "    Args:\n",
    "    - M (np.matrix): matrix to project\n",
    "    - eps (float): tolerance to set negative and zero eigenvalues to\n",
    "\n",
    "    Returns:\n",
    "    - M (np.matrix): projected matrix\n",
    "    \"\"\"\n",
    "    # project a matrix to the positive definite cone\n",
    "    # M: a symmetric matrix\n",
    "    # return: a symmetric matrix\n",
    "    if is_pd(M):\n",
    "        return M\n",
    "\n",
    "    # eigen decomposition\n",
    "    eig_vals, eig_vecs = np.linalg.eig(M)\n",
    "    # set negative eigenvalues to eps\n",
    "    eig_vals[eig_vals <= 0] = eps\n",
    "    # reconstruct matrix\n",
    "    return eig_vecs @ np.diag(eig_vals) @ eig_vecs.T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OAIS(phi, pi, q0, lr, nsamples, niter):\n",
    "    results = []\n",
    "    for _ in range(niter):\n",
    "        # compute inner product\n",
    "        samples = q0.sample(nsamples)\n",
    "        w = pi.pdf(samples) / q0.pdf(samples) # compute w as we have access to pi\n",
    "        w2 = w**2\n",
    "        phi_samples = np.apply_along_axis(phi, 0, samples)\n",
    "        integral = np.mean(w*phi_samples)/np.mean(w)\n",
    "\n",
    "        # update q0\n",
    "        results.append(integral)\n",
    "        partial_m = partial_lq_m(samples, q0)\n",
    "        update_m = -np.mean(w2 * partial_m, axis=1)\n",
    "        \n",
    "        partial_S = partial_lq_S(samples, q0)\n",
    "        update_S = -np.mean(w2*partial_S, axis=2)\n",
    "        \n",
    "        new_S = project_pd(q0.S - lr*update_S)\n",
    "        new_m = q0.m - lr*update_m\n",
    "        q0.update_vals(new_m, new_S)\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5.94752801e-03, 1.38980576e-02, 6.47567485e-03, 5.14608602e-02,\n",
       "       2.86301991e-03, 1.64091639e-02, 1.16757887e-02, 4.44280026e-03,\n",
       "       4.91396452e-04, 2.65784535e-03, 1.81488986e-02, 1.21351200e-02,\n",
       "       1.88690743e-02, 2.54829631e-02, 2.42841599e-02, 1.46844852e-02,\n",
       "       9.56578589e-03, 1.40737871e-02, 1.18417626e-02, 7.94049891e-03,\n",
       "       6.37128793e-03, 2.92701639e-04, 5.67596910e-03, 1.52813023e-02,\n",
       "       6.40132854e-03, 1.41552491e-02, 6.57280330e-03, 1.65068072e-02,\n",
       "       6.80070918e-03, 7.92005249e-03, 4.18897061e-03, 9.59546862e-03,\n",
       "       1.78900450e-02, 2.68980815e-02, 9.34662077e-03, 1.56066261e-02,\n",
       "       2.60442718e-03, 6.40132689e-03, 1.24280506e-03, 1.18878049e-02,\n",
       "       2.63516927e-03, 6.42078106e-03, 1.12593547e-02, 1.11638251e-02,\n",
       "       9.80686276e-03, 4.40609320e-03, 2.08479638e-03, 1.36210738e-02,\n",
       "       1.68901981e-02, 2.11126417e-02, 1.98014011e-02, 7.65251513e-03,\n",
       "       4.57952585e-03, 8.52280197e-03, 1.82682619e-02, 9.27244299e-03,\n",
       "       4.52513046e-03, 9.89038966e-03, 2.03871023e-02, 3.19300207e-03,\n",
       "       1.30161278e-02, 4.37230839e-03, 2.30204996e-02, 1.53944447e-02,\n",
       "       7.24045174e-03, 1.59475658e-03, 2.88997167e-05, 3.16580096e-03,\n",
       "       2.79823979e-02, 3.59392906e-03, 2.04898371e-02, 1.75620782e-02,\n",
       "       1.94732440e-02, 2.14870668e-03, 1.09639348e-02, 4.90867867e-03,\n",
       "       2.12853285e-03, 1.34367705e-02, 7.62980961e-03, 4.68714106e-03,\n",
       "       1.17382471e-03, 1.95805671e-02, 9.79132757e-03, 4.13513708e-03,\n",
       "       1.11067685e-02, 1.72400272e-02, 7.04518902e-03, 6.82035834e-03,\n",
       "       5.19039272e-03, 2.32097218e-02, 7.59137975e-03, 7.07909144e-03,\n",
       "       9.94577807e-03, 1.00293211e-02, 1.56406218e-02, 4.49389403e-03,\n",
       "       4.12766823e-03, 1.14064730e-02, 1.55419903e-02, 3.71149188e-03])"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "        \n",
    "q = NormalRV(np.array([1,1]), np.array([[5,1],[1,5]]))\n",
    "pi = NormalRV(np.array([0,0]), np.array([[1,0],[0,1]]))\n",
    "\n",
    "def phi(x):\n",
    "    # indicator function for square [-0.5, 0.5]^2\n",
    "    if np.abs(x[0]) < 0.5 and np.abs(x[1]) < 0.5:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "e = OAIS(phi, pi, q, 0.01, 1000, 100)\n",
    "\n",
    "v = sp.stats.norm.cdf(0.5) - sp.stats.norm.cdf(-0.5)\n",
    "np.abs(e - v**2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
